{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "Note that, as in the Titanic assignment, the housing pricees dataset is locally stored in the folder structure described in the repository Readme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environmental setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environmental setup\n",
    "import os;  import sys;  import numpy as np;  import pandas as pd;  import sklearn\n",
    "\n",
    "# allow inline display of matplotlib graphs\n",
    "import matplotlib as mpl;   import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# suppress errors, as seen in Lecture 8; see SciPy issue #5998\n",
    "import warnings;   warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "# unify RNG across runs\n",
    "seedNum=0;  np.random.seed(seedNum)\n",
    "\n",
    "# read content of training + test datasets\n",
    "dsamp=pd.read_csv('../Datasets/housePrices/train.csv')\n",
    "\n",
    "# make entry ID into dataframe index\n",
    "dsamp.set_index('Id', inplace=True)\n",
    "\n",
    "print('Data/library loading complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data (re-)organization\n",
    "The dataset contains a large number of variables, some of which are redundant or may be relevant in multiple aspects. Considering that the Amereican housing market is often valued on the availability of specific amenities, several categories were defined, and relevant variables were grouped under them. Principal component analysis will be conducted on each category to reduce the dimensionality of the final input into the eventual ML model.\n",
    "\n",
    "To allow this sort of analysis, a function will, first, be created to easily normalize columns in a dataframe. Since it is known that many variables are non-normal, capabilities to automatically apply log-transforms and the Wilson-Hilferty transform for Rayleigh-distributed data were also included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def normalize(df,a):\n",
    "    for col in range(len(df.columns)):\n",
    "        dfNow=pd.DataFrame( df.iloc[:,col], columns=[df.columns[col]] )\n",
    "        x=dfNow.values.flatten()         # SciPy requires input as 1-D array\n",
    "        z1,p1=stats.normaltest(x)\n",
    "        if p1<a and isinstance(x,float): # attempt 1: log-transform\n",
    "            x=np.log(x+abs(min(x))+1)\n",
    "            z2,p2=stats.normaltest(x)\n",
    "            if p2<a:                     # attempt 2: Wilson-Hilferty transform for Rayleigh distr.\n",
    "                x=x**(2/3)\n",
    "        df.iloc[:,col]=x\n",
    "    return pd.DataFrame(df, index=df.index, columns=df.columns)\n",
    "\n",
    "alpha=0.01;   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geography\n",
    "First, variables pertaining to the geography of each listing were extracted and modified. This includes information about both the property in its local environment (e.g. terrain, adjacent roads) as well as its context in Ames, IA as a whole.\n",
    "\n",
    "| Variable Name | Variable Meaning             | Type    | Discard? | Convert to: |\n",
    "| :---          | :---                         | :---    | :---     | :---        |\n",
    "| MSZoning      | Zoning classification        | object  | Partial  | see note 01 |\n",
    "| LotFrontage   | Length of curb @ prop. (ft)  | object  |          | Integer     |\n",
    "| Street        | Road access type             | object  | YES      |             |\n",
    "| Alley         | Alley access type            | object  | YES      |             |\n",
    "| LotShape      | Property shape irregularity  | object  |          | Ordinal     |\n",
    "| LandContour   | Property flatness            | object  |          | 1-hot enc.  |\n",
    "| LotConfig     | Lot location wrt streetfront | object  |          | 1-hot enc.  |\n",
    "| LandSlope     | Slope of property            | object  |          | Ordinal     |\n",
    "| Neighborhood  | Neighborhood in Ames, IA     | object  |          | Ordinal     |\n",
    "| Condition1    | Proximity to various places  | object  | Partial  | see note 02 |\n",
    "| Condition2    | - see above -                | object  | Partial  | see note 02 |\n",
    "| YearBuilt     | Year of original construc.   | int64   |          |             |\n",
    "\n",
    "The street, alley categories were removed, as those categories were nearly homogeneous in the training dataset.\n",
    "\n",
    "The neighborhood location was converted into ordinal values based on each neighborhood's median home value.\n",
    "\n",
    "**Note 01:** Zoning classifications were transformed differently for each variable. Categories that are not formally residential -Agriculture, Commercial, and Industrial- were deleted. The remaining \"Residential\" zones were ranked ordinally, with \"Low Density Park\" being ranked in between \"Low\" and \"Medium\" densities and \"Floating Village Residential\" being treated below \"Low\" density rank.\n",
    "\n",
    "**Note 02:** Conditions of a property are ordinally encoded based on the following categories:\n",
    "\n",
    "* Automobile Traffic (Artery > Feedr > n/a)\n",
    "* East-West Rail (RRAe > RRNe > n/a)\n",
    "* North-South Rail (RRAn > RRNn > n/a)\n",
    "* \"Positive\" Off-site Feature (PosA > PosN > n/a)\n",
    "\n",
    "\"Norm\" was eliminated due to redundancy. Note that the distinction between the North-South and East-West rails were preserved. This is because the track usage for these two rail rights-of-way may be significantly different, considering significant differences in how they contribute to rail owner Union Pacific's strategy (also see https://iowadot.gov/iowainmotion/railplan/2017/IowaSRP2017_AppendixA.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas=dsamp['Neighborhood'].unique()                # list of neighborhoods\n",
    "subdivID=range(len(areas))                          # get IDs for neighborhoods\n",
    "\n",
    "medians=np.ndarray(len(areas),)\n",
    "for n in range(len(areas)):\n",
    "    x=dsamp['Neighborhood'].str.contains(areas[n])  # \"dsamp\" entries for current subdivision\n",
    "    medians[n]=np.median( dsamp.loc[x, 'SalePrice'].to_numpy() ) # median home sale price\n",
    "\n",
    "medseq=[x for x,y in sorted(enumerate(medians),     # index of neighborhood labels, in order of\n",
    "                            key = lambda x: x[1])]  # increasing median house market value\n",
    "\n",
    "fig=plt.figure(figsize=(12, 6))                     # initialize figure\n",
    "ax=fig.add_axes([0,0,1,1])\n",
    "xPos=0\n",
    "for n in medseq:   \n",
    "    x=dsamp['Neighborhood'].str.contains(areas[n])  # \"dsamp\" entries for current subdivision\n",
    "    X=dsamp.loc[x, 'SalePrice'].to_numpy()          # all home sale price in above subdiv.\n",
    "    vp=ax.violinplot(X, [xPos], points=80, vert=True, widths=0.7, showextrema=True, showmedians=True)\n",
    "    xPos=xPos+1                                     # manual change in plot position\n",
    "\n",
    "\n",
    "plt.xticks(range(len(areas)), areas, rotation='vertical')\n",
    "plt.grid(color='k', linewidth=1, alpha=0.05)\n",
    "plt.show()\n",
    "\n",
    "def org_Neighborhood(df_in):\n",
    "    df_in=df_in[['Neighborhood']]\n",
    "    return pd.get_dummies(df_in, prefix=\"nghb\", dtype=\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def org_MSZoning(df_in):\n",
    "    df_in=df_in[['MSZoning']]\n",
    "    df_zones=pd.get_dummies(df_in, prefix=\"zone\", dtype=\"int64\")\n",
    "    if 'zone_FV' not in df_zones: df_zones['zone_FV']=0 # contingency, float. vill.\n",
    "    if 'zone_RP' not in df_zones: df_zones['zone_RP']=0 # contingency, res.-park\n",
    "    df_zones['zone_res']=\\\n",
    "     df_zones['zone_FV'] + 2*df_zones['zone_RP'] +\\\n",
    "     3*df_zones['zone_RM'] + 4*df_zones['zone_RH']  # ordinal encoding \"residential\"s\n",
    "    return df_zones[['zone_res']]\n",
    "\n",
    "def org_LotFrontage(df_in):\n",
    "    df_in=df_in[['LotFrontage']]\n",
    "    cleanNaN=lambda x: 0.0 if (x is None or np.isnan(x)) else float(x)\n",
    "    df_out=df_in['LotFrontage'].apply(cleanNaN)\n",
    "    return df_out.to_frame(name='frntg')\n",
    "\n",
    "def org_LotShape(df_in):\n",
    "    df_in=df_in[['LotShape']]\n",
    "    irreg={'Reg':0, 'IR1':1, 'IR2':2, 'IR3':3}\n",
    "    return normalize( df_in['LotShape'].replace(irreg).to_frame(name='irreg').astype(int), alpha)\n",
    "\n",
    "def org_LandContour(df_in):\n",
    "    df_in=df_in[['LandContour']]\n",
    "    return pd.get_dummies(df_in, prefix=\"ctur\", dtype=\"int64\")\n",
    "\n",
    "def org_LotConfig(df_in):\n",
    "    df_in=df_in[['LotConfig']]\n",
    "    return pd.get_dummies(df_in, prefix=\"conf\", dtype=\"int64\")\n",
    "\n",
    "def org_LandSlope(df_in):\n",
    "    df_in=df_in[['LandSlope']]\n",
    "    irreg={'Gtl':0, 'Mod':1, 'Sev':2}\n",
    "    return normalize( df_in['LandSlope'].replace(irreg).to_frame(name='slope').astype(int), alpha)\n",
    "\n",
    "def org_Condition(df_in):\n",
    "    df1=df_in[['Condition1']];  df2=df_in[['Condition2']]\n",
    "    df1=pd.get_dummies(df1, prefix=\"cond\", dtype=\"int64\")  # 1-hot, condition 1\n",
    "    df2=pd.get_dummies(df2, prefix=\"cond\", dtype=\"int64\")  # 1-hot, condition 2\n",
    "    df1.drop(columns=['cond_Norm']);   df2.drop(columns=['cond_Norm'])\n",
    "    conds=['cond_Artery', 'cond_Feedr', 'cond_RRNn',   'cond_RRAn',\\\n",
    "           'cond_RRNe',   'cond_RRAe',  'cond_PosN',   'cond_PosA']\n",
    "    for cond in conds:\n",
    "        if cond not in df1: df1[cond]=0\n",
    "        if cond not in df2: df2[cond]=0\n",
    "    df_all=df1+df2\n",
    "    df_all['cond_traf']=df_all['cond_Feedr']+ 2*df_all['cond_Artery']\n",
    "    df_all['cond_RRns']=df_all['cond_RRNn'] + 2*df_all['cond_RRAn']\n",
    "    df_all['cond_RRew']=df_all['cond_RRNe'] + 2*df_all['cond_RRAe']\n",
    "    df_all['cond_qol']= df_all['cond_PosN'] + 2*df_all['cond_PosA']\n",
    "    return df_all[['cond_traf','cond_RRns','cond_RRew','cond_qol']].astype(int)\n",
    "\n",
    "def org_geography(df_in):\n",
    "    df=pd.concat([org_MSZoning(df_in),\\\n",
    "                  org_LotConfig(df_in),org_LotShape(df_in),\\\n",
    "                  org_LotFrontage(df_in),org_LotShape(df_in),\\\n",
    "                  org_LandContour(df_in),org_LandSlope(df_in),\\\n",
    "                  org_Neighborhood(df_in),org_Condition(df_in),\\\n",
    "                  df_in[['YearBuilt']]\\\n",
    "                 ], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "Next, variables concerning the structure of the houses were isolated.\n",
    "\n",
    "#### General\n",
    "The following variables, all of which encode information about the general structure of the property in question, were modified together.\n",
    "\n",
    "| Variable Name | Variable Meaning             | Type    | Discard? | Convert to: |\n",
    "| :---          | :---                         | :---    | :---     | :---        |\n",
    "| YearBuilt     | Year of original construc.   | int64   |          |             |\n",
    "| YearRemodAdd  | Remodel yr. (=built if none) | int64   |          | see note 03 |\n",
    "| LotArea       | Proprety area (sq ft)        | int64   |          |             |\n",
    "| BldgType      | Type of dwelling             | object  |          | 1-hot enc.  |\n",
    "| MSSubClass    | Type of dwelling             | int64   | Partial  | see note 04 |\n",
    "| HouseStyle    | Style of dwelling (# floor)  | object  |          | see note 05 |\n",
    "| OverallQual   | Rating of material/quality   | int64   |          |             |\n",
    "| OverallCond   | Rating of house condition    | int64   |          |             |\n",
    "| Foundation    | Foundation type              | object  |          | 1-hot enc.  |\n",
    "\n",
    "**Note 03:** Remodeling years will be preserved as integers, but the difference between original and remodeled years will be taken to imply whether a house was rebuilt at all.\n",
    "\n",
    "**Note 04:** Class 190 (2-family conversion) and 120-180 (Planned Unit Development) properties will be one-hot encoded. Other subclasses will be eliminated, as they are redundant to other variables.\n",
    "\n",
    "**Note 05:** Floors (1, 1.5, 2, or 2.5) will be stored as ordinal ranks (1 to 4), with \"Split Foyer\" being classified as a 1.5-floor property and \"Split Level\" houses being treeated as a two-floor property. Finished and unfinished floors are not distinguished here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def org_years(df_in):\n",
    "    df_ori=df_in[['YearBuilt']]\n",
    "    df_mod=df_in[['YearRemodAdd']] - df_ori.values\n",
    "    return pd.concat([df_ori, df_mod], axis=1)\n",
    "\n",
    "def org_BldgType(df_in):\n",
    "    df_in=df_in[['BldgType']]\n",
    "    return pd.get_dummies(df_in, prefix=\"type\", dtype=\"int64\")\n",
    "\n",
    "def org_MSSubClass(df_in):\n",
    "    df_in=df_in[['MSSubClass']].copy()    # explicitly allocate memory\n",
    "    cond1=df_in.MSSubClass < 120          # combine subclasses into categories\n",
    "    cond2=( df_in['MSSubClass'] >=120 )&\\\n",
    "          ( df_in['MSSubClass'] < 190 )\n",
    "    cond3=df_in.MSSubClass==190\n",
    "    df_in.loc[cond1, 'MSSubClass']='std'  # make groups into strings\n",
    "    df_in.loc[cond2, 'MSSubClass']='PUD'\n",
    "    df_in.loc[cond3, 'MSSubClass']='2FC'\n",
    "    df_out=pd.get_dummies(df_in, prefix=\"subc\", dtype=\"int64\")\n",
    "    return df_out.drop(['subc_std'], axis=1).astype(int)\n",
    "\n",
    "def HouseStyle(df_in):\n",
    "    df_in=df_in[['HouseStyle']]\n",
    "    df_zones=pd.get_dummies(df_in, prefix=\"styl\", dtype=\"int64\")\n",
    "    conds=['styl_1Story', 'styl_1.5Fin', 'styl_1.5Unf',\\\n",
    "           'styl_2Story', 'styl_2.5Fin', 'styl_2.5Unf',\\\n",
    "           'styl_SFoyer', 'styl_SLvl']\n",
    "    resps=[1, 2, 2, 3, 4, 4, 2, 3]        # indicators corresponding to \"conds\"\n",
    "    for cond in np.arange(len(conds)):\n",
    "        if conds[cond] in df_zones:\n",
    "            df_zones.loc[ df_zones[conds[cond]]==1, 'Style']=resps[cond]\n",
    "        else:\n",
    "            df_zones['Style']=0\n",
    "    return df_zones[['Style']].astype(int)\n",
    "\n",
    "def org_Foundation(df_in):\n",
    "    df_in=df_in[['Foundation']]\n",
    "    return pd.get_dummies(df_in, prefix=\"fndt\", dtype=\"int64\")\n",
    "\n",
    "def org_strGen(df_in):\n",
    "    df=pd.concat([org_years(df_in),df_in[['LotArea']],\\\n",
    "                  org_MSSubClass(df_in),HouseStyle(df_in),\\\n",
    "                  df_in[['OverallQual','OverallCond']],\\\n",
    "                  org_Foundation(df_in)\\\n",
    "                 ], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exterior\n",
    "Next, information on the house's exterior were modified as follows.\n",
    "\n",
    "| Variable Name | Variable Meaning             | Type    | Discard? | Convert to: |\n",
    "| :---          | :---                         | :---    | :---     | :---        |\n",
    "| RoofStyle     | Roof geometry                | object  |          | 1-hot enc.  |\n",
    "| RoofMatl      | Roof material                | object  | YES      |             |\n",
    "| Exterior1st   | Exterior material            | object  |          | 1-hot enc.  |\n",
    "| Exterior2nd   | - see above -                | object  |          | 1-hot enc.  |\n",
    "| MasVnrType    | Masonry veneer type          | object  | YES      | see note 06 |\n",
    "| MasVnrArea    | Masonry veneer area (sq ft)  | object  |          | see note 06 |\n",
    "| ExterQual     | Exterior material quality    | object  |          | Ordinal     |\n",
    "| ExterCond     | Exterior condition           | object  |          | Ordinal     |\n",
    "\n",
    "The roof material category was removed, as the vast majority of the training dataset belonged in the composite shingles group.\n",
    "\n",
    "**Note 06:** Masonry veneer area will be encoded as their own integers, separated by masonry type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def org_RoofStyle(df_in):\n",
    "    df_in=df_in[['RoofStyle']]\n",
    "    return pd.get_dummies(df_in, prefix=\"rstyle\", dtype=\"int64\")\n",
    "\n",
    "def org_Exterior(df_in):\n",
    "    df1=df_in[['Exterior1st']].copy()                     # explicitly allocate memory\n",
    "    df2=df_in[['Exterior2nd']].copy()\n",
    "    df2[ df2=='Brk Cmn' ]='BrkComm'                       # correct inconsistent labels\n",
    "    df2[ df2=='CmentBd' ]='CemntBd'\n",
    "    df2[ df2=='Wd Shng' ]='WdShing'\n",
    "    df1=pd.get_dummies(df1, prefix=\"ext\", dtype=\"int64\")  # 1-hot encode each mat type\n",
    "    df2=pd.get_dummies(df2, prefix=\"ext\", dtype=\"int64\")\n",
    "    dfAll=df1.add(df2, axis=1, fill_value=0)\n",
    "    conds=['ext_AsbShng', 'ext_AsphShn', 'ext_BrkComm','ext_BrkFace', 'ext_CBlock',\\\n",
    "           'ext_CemntBd','ext_HdBoard', 'ext_ImStucc','ext_MetalSd', 'ext_Other',\\\n",
    "           'ext_Plywood', 'ext_PreCast', 'ext_Stone',  'ext_Stucco',  'ext_VinylSd',\\\n",
    "           'ext_Wd Sdng', 'ext_WdShing']\n",
    "    for cond in conds:\n",
    "        if cond not in dfAll: dfAll[cond]=0\n",
    "    return dfAll.astype(int)\n",
    "    \n",
    "def org_masonry(df_in):\n",
    "    df_type=pd.get_dummies(df_in[['MasVnrType']], prefix=\"masstyle\", dtype=\"int64\")\n",
    "    df_type.drop(labels='masstyle_None', axis=1, inplace=True)\n",
    "    df_area=df_in[['MasVnrArea']]\n",
    "    df_area.fillna(value=0, inplace=True)\n",
    "    return pd.DataFrame(df_area.values*df_type.values,\\\n",
    "                        columns=df_type.columns, index=df_area.index, dtype=int)\n",
    "\n",
    "def org_ExterQual(df_in):\n",
    "    df_in=df_in[['ExterQual']]\n",
    "    exqu={'Po':-2, 'Fa':-1, 'TA':0, 'Gd':1, 'Ex':2}\n",
    "    return df_in['ExterQual'].replace(exqu).to_frame(name='extQual').astype(int)\n",
    "\n",
    "def org_ExterCond(df_in):\n",
    "    df_in=df_in[['ExterCond']]\n",
    "    exco={'Po':-2, 'Fa':-1, 'TA':0, 'Gd':1, 'Ex':2}\n",
    "    return df_in['ExterCond'].replace(exco).to_frame(name='extCond').astype(int)\n",
    "\n",
    "def org_exteriors(df_in):\n",
    "    df=pd.concat([org_RoofStyle(df_in),\\\n",
    "                  org_Exterior(df_in),  org_masonry(df_in),\\\n",
    "                  org_ExterQual(df_in), org_ExterCond(df_in)\\\n",
    "                 ], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basement\n",
    "Likewise, information on a property's basement was encoded as follows. Note that properties without a basement were assigned zero or median values as appropriate.\n",
    "\n",
    "| Variable Name | Variable Meaning             | Type    | Discard? | Convert to: |\n",
    "| :---          | :---                         | :---    | :---     | :---        |\n",
    "| BsmtQual      | Basement height, categorized | object  |          | see note 07 |\n",
    "| BsmtCond      | Basement condition           | object  |          | Ordinal     |\n",
    "| BsmtExposure  | Walkout/garden-level walls?  | object  |          | 1-hot enc.  |\n",
    "| BsmtFinType1  | Rating, finished basement    | object  |          | see note 08 |\n",
    "| BsmtFinSF1    | above, square footage        | int64   | YES      | see note 09 |\n",
    "| BsmtFinType2  | ditto, multiple types        | object  |          | see note 08 |\n",
    "| BsmtFinSF2    | see above                    | int64   | YES      | see note 09 |\n",
    "| BsmtUnfSF     | Unfinished basement sq ft    | int64   |          |             |\n",
    "| TotalBsmtSF   | Total basement sq ft         | int64   | YES      |             |\n",
    "| BsmtFullBath  | Basement full bathroom       | int64   |          |             |\n",
    "| BsmtHalfBath  | Basement half bathroom       | int64   |          |             |\n",
    "\n",
    "**Note 07:** Basement height will be categorized based on whether it does NOT have a low ceiling. Properties with no basement will be classifieed as \"no\".\n",
    "\n",
    "**Note 08:** One-hot encoding was applied to both of these entries, together.\n",
    "\n",
    "**Note 09:** The square footage will bee combined with the basement finish types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def org_BsmtQual(df_in):\n",
    "    df_in=df_in[['BsmtQual']]\n",
    "    df_in.fillna(value='NA', inplace=True)\n",
    "    bsqu={'NA':0, 'Po':0, 'Fa':0, 'TA':1, 'Gd':1, 'Ex':1}\n",
    "    return df_in['BsmtQual'].replace(bsqu).to_frame(name='bsmtQual').astype(int)\n",
    "\n",
    "def org_BsmtCond(df_in):\n",
    "    df_in=df_in[['BsmtCond']]\n",
    "    df_in.fillna(value='NA', inplace=True)\n",
    "    bsco={'NA':0, 'Po':-2, 'Fa':-1, 'TA':0, 'Gd':1, 'Ex':2}\n",
    "    return df_in['BsmtCond'].replace(bsco).to_frame(name='bsmtCond').astype(int)\n",
    "\n",
    "def org_BsmtExposure(df_in):\n",
    "    df_in=df_in[['BsmtExposure']]\n",
    "    df_in.fillna(value='NA', inplace=True)\n",
    "    bsex={'NA':0, 'No':0, 'Mn':1, 'Av':2, 'Gd':3}\n",
    "    return df_in['BsmtExposure'].replace(bsex).to_frame(name='bsmtExpo').astype(int)\n",
    "\n",
    "def org_BsmtFinType(df_in):\n",
    "    dfT1=pd.get_dummies(df_in[['BsmtFinType1']], prefix=\"bsfin\", dtype=\"int64\")\n",
    "    dfT2=pd.get_dummies(df_in[['BsmtFinType2']], prefix=\"bsfin\", dtype=\"int64\")\n",
    "    dfF1=pd.get_dummies(df_in[['BsmtFinSF1']],   prefix=\"bs_sf\", dtype=\"int64\")\n",
    "    dfF2=pd.get_dummies(df_in[['BsmtFinSF2']],   prefix=\"bs_sf\", dtype=\"int64\")\n",
    "    dfT1.fillna(value=0, inplace=True)\n",
    "    dfT2.fillna(value=0, inplace=True)\n",
    "    dfF1.fillna(value=0, inplace=True)\n",
    "    dfF2.fillna(value=0, inplace=True)\n",
    "    df1=pd.DataFrame( dfT1.values * dfF1.values,\\\n",
    "                      columns=dfT1.columns, index=dfT1.index, dtype=int)\n",
    "    df2=pd.DataFrame( dfT2.values * dfF2.values,\\\n",
    "                      columns=dfT2.columns, index=dfT2.index, dtype=int)\n",
    "    return pd.DataFrame( df1.values + df2.values,\\\n",
    "                      columns=df1.columns, index=df1.index, dtype=int)\n",
    "\n",
    "def org_TotalBsmtSF(df_in):\n",
    "    df_in=df_in[['TotalBsmtSF']]\n",
    "    df_in.fillna(value=0, inplace=True)\n",
    "    return df_in.astype(int)\n",
    "\n",
    "def org_BsmtBaths(df_in):\n",
    "    df_in=df_in[['BsmtFullBath','BsmtHalfBath']]\n",
    "    df_in.fillna(value=0, inplace=True)\n",
    "    return df_in.astype(int)\n",
    "\n",
    "def org_basement(df_in):\n",
    "    df=pd.concat([org_BsmtQual(df_in),     org_BsmtCond(df_in),\\\n",
    "                  org_BsmtExposure(df_in), org_BsmtFinType(df_in),\\\n",
    "                  org_TotalBsmtSF(df_in),  org_BsmtBaths(df_in)\\\n",
    "                 ], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HVAC and Utilities\n",
    "Information on a property's heating, ventilation, air conditioning, electrical systems, and fireplacs were also considered and modified as follows.\n",
    "\n",
    "| Variable Name | Variable Meaning             | Type    | Discard? | Convert to: |\n",
    "| :---          | :---                         | :---    | :---     | :---        |\n",
    "| Utilities     | Utility types missing        | object  | YES      |             |\n",
    "| Heating       | Heating type                 | object  | YES      |             |\n",
    "| HeatingQC     | Heating quality/condition    | object  |          | Ordinal     |\n",
    "| CentralAir    | Central AC?                  | object  |          | int64       |\n",
    "| Electrical    | Electrical system quality    | object  |          | see note 10 |\n",
    "| Fireplaces    | Fireplace count              | int64   |          |             |\n",
    "| FireplaceQu   | Fireplace quality            | object  |          | Ordinal     |\n",
    "\n",
    "While the Utilities category should, ideally, be a critical indicator for the value of a property listing, nearly all datapoints in the training dataset contained properties with full access to public utilities. Thus, this category was omitted from analysis.\n",
    "\n",
    "**Note 10:** The quality of the electrical system will be listed ordinally, as described in the data description file. A \"mixed\" entry will be treated identically to a \"FuseA\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def org_HeatingQC(df_in):\n",
    "    df_in=df_in[['HeatingQC']]\n",
    "    hequ={'Po':-2, 'Fa':-1, 'TA':0, 'Gd':1, 'Ex':2}\n",
    "    return df_in['HeatingQC'].replace(hequ).to_frame(name='HeatQual').astype(int)\n",
    "\n",
    "def org_CentralAir(df_in):\n",
    "    df_in=df_in[['CentralAir']]\n",
    "    pool={'N':0, 'Y':1}\n",
    "    return df_in['CentralAir'].replace(pool).to_frame(name='centAir').astype(int)\n",
    "\n",
    "def org_Electrical(df_in):\n",
    "    df_in=df_in[['Electrical']]\n",
    "    df_in.fillna(value='SBrkr', inplace=True)\n",
    "    elec={'SBrkr':0, 'FuseA':1, 'FuseF':2, 'FuseP':3, 'Mix':1}\n",
    "    return df_in['Electrical'].replace(elec).to_frame(name='electr').astype(int)\n",
    "\n",
    "def org_FireplaceQu(df_in):\n",
    "    df_in=df_in[['FireplaceQu']]\n",
    "    df_in.fillna(value='NA', inplace=True)\n",
    "    fpqu={'NA':0, 'Po':-2, 'Fa':-1, 'TA':0, 'Gd':1, 'Ex':2}\n",
    "    return df_in['FireplaceQu'].replace(fpqu).to_frame(name='FireQual').astype(int)\n",
    "\n",
    "def org_util(df_in):\n",
    "    df=pd.concat([org_HeatingQC(df_in),\\\n",
    "                  org_CentralAir(df_in), org_Electrical(df_in),\\\n",
    "                  df_in[['Fireplaces']], org_FireplaceQu(df_in)\\\n",
    "                 ], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above-Grade Internals\n",
    "In contrast with below-grade (below-surface level) structures in the Basement section, above-ground internal features of a property listing are listed here.\n",
    "\n",
    "| Variable Name | Variable Meaning             | Type    | Discard? | Convert to: |\n",
    "| :---          | :---                         | :---    | :---     | :---        |\n",
    "| 1stFlrSF      | 1F square footage            | int64   |          |             |\n",
    "| 2ndFlrSF      | 2F square footage            | int64   |          |             |\n",
    "| LowQualFinSF  | Lo-qual. finished sq ft      | int64   | YES      |             |\n",
    "| GrLivArea     | Above-grade living area sqft | int64   |          |             |\n",
    "| FullBath      | Above-grade full bathroom    | int64   |          |             |\n",
    "| HalfBath      | Above-grade half bathroom    | int64   |          |             |\n",
    "| BedroomAbvGr  | Above-grade bedrooms         | int64   |          |             |\n",
    "| KitchenAbvGr  | Above-grade kitchen          | int64   |          |             |\n",
    "| KitchenQual   | Kitchen quality              | object  |          | Ordinal     |\n",
    "| TotRmsAbvGrd  | Above-grade rooms            | int64   | YES      |             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def org_KitchenQual(df_in):\n",
    "    df_in=df_in[['KitchenQual']]\n",
    "    df_in.fillna(value='TA', inplace=True)\n",
    "    kiqu={'Po':-2, 'Fa':-1, 'TA':0, 'Gd':1, 'Ex':2}\n",
    "    return df_in['KitchenQual'].replace(kiqu).to_frame(name='KitcQual').astype(int)\n",
    "\n",
    "def org_above(df_in):\n",
    "    df=pd.concat([df_in[['1stFlrSF','2ndFlrSF','GrLivArea',\\\n",
    "                         'FullBath','HalfBath','BedroomAbvGr',\\\n",
    "                         'KitchenAbvGr']],\\\n",
    "                  org_KitchenQual(df_in)\\\n",
    "                 ], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Garage\n",
    "Information on the garage (if present; otherwise zero or median-impute) were modified as follows.\n",
    "\n",
    "| Variable Name | Variable Meaning             | Type    | Discard? | Convert to: |\n",
    "| :---          | :---                         | :---    | :---     | :---        |\n",
    "| GarageType    | Garage location              | object  |          | 1-hot enc.  |\n",
    "| GarageYrBlt   | Year of garage completion    | int64   |          |             |\n",
    "| GarageFinish  | Interior finish of garage    | object  |          | ordinal     |\n",
    "| GarageCars    | Max num cars in garage       | int64   |          |             |\n",
    "| GarageArea    | Garage sq ft                 | int64   | YES      |             |\n",
    "| GarageQual    | Garage quality               | object  |          | ordinal     |\n",
    "| GarageCond    | Garage condition             | object  |          | ordinal     |\n",
    "| PavedDrive    | Paved driveway?              | object  |          | ordinal     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def org_GarageType(df_in):\n",
    "    df_in=df_in[['GarageType']]\n",
    "    return pd.get_dummies(df_in, prefix=\"gtype\", dtype=\"int64\")\n",
    "\n",
    "def org_GarageYrBlt(df_in):\n",
    "    df_in=df_in[['GarageYrBlt']]\n",
    "    df_in['GarageYrBlt'].fillna(1965, inplace=True) # impute w/ approximate mode\n",
    "    return df_in[['GarageYrBlt']].astype(float)\n",
    "\n",
    "def org_GarageCars(df_in):\n",
    "    df_in=df_in[['GarageCars']]\n",
    "    df_in.replace(to_replace=[None], value=0, inplace=True)#contingency\n",
    "    df_in.fillna(value=0, inplace=True)\n",
    "    return df_in.astype(int)\n",
    "\n",
    "def org_GarageFinish(df_in):\n",
    "    df_in=df_in[['GarageFinish']]\n",
    "    df_in['GarageFinish'].fillna('NA', inplace=True)\n",
    "    pool={'NA':0, 'Unf':1, 'RFn':2, 'Fin':3}\n",
    "    return df_in['GarageFinish'].replace(pool).to_frame(name='gfin').astype(int)\n",
    "\n",
    "def org_GarageQual(df_in):\n",
    "    df_in=df_in[['GarageQual']]\n",
    "    df_in['GarageQual'].fillna('NA', inplace=True)\n",
    "    pool={'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}\n",
    "    return df_in['GarageQual'].replace(pool).to_frame(name='gqual').astype(int)\n",
    "\n",
    "def org_GarageCond(df_in):\n",
    "    df_in=df_in[['GarageCond']]\n",
    "    df_in['GarageCond'].fillna('NA', inplace=True)\n",
    "    pool={'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}\n",
    "    return df_in['GarageCond'].replace(pool).to_frame(name='gcond').astype(int)\n",
    "\n",
    "def org_PavedDrive(df_in):\n",
    "    df_in=df_in[['PavedDrive']]\n",
    "    df_in['PavedDrive'].fillna('NA', inplace=True)\n",
    "    pool={'N':0, 'P':1, 'Y':2}\n",
    "    return df_in['PavedDrive'].replace(pool).to_frame(name='gpave').astype(int)\n",
    "\n",
    "def org_garage(df_in):\n",
    "    df=pd.concat([org_GarageType(df_in),org_GarageYrBlt(df_in),org_GarageCars(df_in),\\\n",
    "                  org_GarageFinish(df_in), org_GarageQual(df_in),\\\n",
    "                  org_GarageCond(df_in), org_PavedDrive(df_in)\\\n",
    "                 ], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porch/Yard Enhancements\n",
    "Properties with a porch or yard, as well as relevant features, were modified as follows.\n",
    "\n",
    "| Variable Name | Variable Meaning             | Type    | Discard? | Convert to: |\n",
    "| :---          | :---                         | :---    | :---     | :---        |\n",
    "| WoodDeckSF    | Wood deck area, sq ft        | int64   |          |             |\n",
    "| OpenPorchSF   | Open porch area, sq ft       | int64   |          |             |\n",
    "| EnclosedPorch | Enclosed porch area, sq ft   | int64   |          |             |\n",
    "| 3SsnPorch     | 3-screen porch area, sq ft   | int64   |          |             |\n",
    "| ScreenPorch   | Screened porch area, sq ft   | int64   |          |             |\n",
    "| PoolArea      | Pool area, sq ft             | int64   |          |             |\n",
    "| PoolQC        | Pool quality                 | object  | YES      |             |\n",
    "| Fence         | Fence quality                | object  | YES      |             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def org_yard(df_in):\n",
    "    df=df_in[['WoodDeckSF','OpenPorchSF','EnclosedPorch',\\\n",
    "              '3SsnPorch','ScreenPorch','PoolArea']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Features\n",
    "Extra or luxury features that enhance quality of life beyond the expectations of a normal house are categorized here.\n",
    "\n",
    "| Variable Name | Variable Meaning             | Type    | Discard? | Convert to: |\n",
    "| :---          | :---                         | :---    | :---     | :---        |\n",
    "| MiscFeature   | Other features               | object  | YES      |             |\n",
    "| MiscVal       | Dollar value of above        | object  | see note | int64       |\n",
    "\n",
    "**Note 11:** The dollar value of miscellaneous features will be removed from the analysis. Instead, its valuation will be added directly onto the predicted property value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def org_separateMisc(df_in):\n",
    "    dfo=df_in[['SalePrice']].copy()\n",
    "    dfi=df_in[['MiscVal']].copy()\n",
    "    df=pd.concat([df_in.drop(labels='SalePrice', axis=1, inplace=True),\\\n",
    "                  pd.DataFrame(dfo.values - dfi.values, \\\n",
    "                               columns=['BaseVal'], index=dfo.index, dtype=float)\\\n",
    "                 ], axis=1)\n",
    "    return df\n",
    "\n",
    "def org_considerMisc(df_base, df_misc):\n",
    "    dfo=df_base[['BaseVal']].copy()\n",
    "    df_misc=df_misc[['MiscVal']].copy()\n",
    "    df=pd.DataFrame( dfo.values + df_misc.values, \\\n",
    "                     columns=['SalePrice'], index=dfo.index, dtype=float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales Condition\n",
    "Finally, information that considers the transaction of a property, as well as liabilities that directly affect it, are listed.\n",
    "\n",
    "#### Sales\n",
    "Basic information on how and when a property is sold is encoded here.\n",
    "\n",
    "| Variable Name | Variable Meaning             | Type    | Discard? | Convert to: |\n",
    "| :---          | :---                         | :---    | :---     | :---        |\n",
    "| MoSold        | Month sold (MM)              | int64   |          | 1-hot enc.  |\n",
    "| YrSold        | Year sold (YYYY)             | int64   |          |             |\n",
    "| SaleType      | Type of sale                 | object  |          | 1-hot enc.  |\n",
    "| SaleCondition | Condition of sale            | object  |          | 1-hot enc.  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def org_MoSold(df_in):\n",
    "    df_in=df_in[['Foundation']]\n",
    "    return pd.get_dummies(df_in, prefix=\"smonth\", dtype=\"int64\")\n",
    "\n",
    "def org_SaleType(df_in):\n",
    "    df_in=df_in[['SaleType']]\n",
    "    return pd.get_dummies(df_in, prefix=\"stype\", dtype=\"int64\")\n",
    "\n",
    "def org_SaleCondition(df_in):\n",
    "    df_in=df_in[['SaleCondition']]\n",
    "    return pd.get_dummies(df_in, prefix=\"scond\", dtype=\"int64\")\n",
    "\n",
    "def org_sales(df_in):\n",
    "    df=pd.concat([org_MoSold(df_in),\\\n",
    "                  normalize( df_in[['YrSold']].astype(int), alpha ),\\\n",
    "                  org_SaleType(df_in), org_SaleCondition(df_in)\\\n",
    "                 ], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faults\n",
    "The following variables encode faults, liabilities, and compromised features that, intuitively, negatively affect a property's value.\n",
    "\n",
    "| Variable Name | Variable Meaning             | Type    | Discard? | Convert to: |\n",
    "| :---          | :---                         | :---    | :---     | :---        |\n",
    "| OverallQual   | Rating of material/quality   | int64   |          |             |\n",
    "| OverallCond   | Rating of house condition    | int64   |          |             |\n",
    "| Functional    | Deductions for home function | object  |          | Ordinal     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def org_Functional(df_in):\n",
    "    df_in=df_in[['Functional']]\n",
    "    df_in['Functional'].fillna('Min2', inplace=True) # assume minor damages for entries w/o fault ratings\n",
    "    fault={'Typ':0, 'Min1':1, 'Min2':2, 'Mod':3, 'Maj1':4, 'Maj2':5, 'Sev':6, 'Sal:':7}\n",
    "    df_in=df_in['Functional'].replace(fault).to_frame(name='fault')\n",
    "    return normalize( df_in, alpha )\n",
    "\n",
    "def org_faults(df_in):\n",
    "    df=pd.concat([normalize( df_in[['OverallQual','OverallCond']].astype(float), alpha),\\\n",
    "                  org_Functional(df_in)\\\n",
    "                 ], axis=1)\n",
    "    return normalize(df,alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing Everything Together\n",
    "Now that we have (finally!!) defined the functions that will re-format our data in helpful ways, we will actually apply them to our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize(df_in):\n",
    "    return pd.concat([org_geography(df_in), org_strGen(df_in),\\\n",
    "                      org_exteriors(df_in), org_basement(df_in),\\\n",
    "                      org_util(df_in), org_above(df_in), org_garage(df_in),\\\n",
    "                      org_yard(df_in), org_sales(df_in), org_faults(df_in)\\\n",
    "                     ], axis=1)\n",
    "\n",
    "# remove misc. values from true dollar value (log-normalized)\n",
    "df=dsamp[['MiscVal', 'SalePrice']].copy()\n",
    "truth=np.log( org_separateMisc(df).values.ravel() )\n",
    "\n",
    "# organize training and testing datsets\n",
    "ds=organize(dsamp)\n",
    "\n",
    "# impose homoskedasticity\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler=RobustScaler().fit(ds.values)\n",
    "ds_scaled=scaler.transform(ds.values)\n",
    "\n",
    "# convert scaled inputs to DataFrames\n",
    "ds=pd.DataFrame(ds_scaled, index=ds.index, columns=ds.columns)\n",
    "\n",
    "print('Data organization complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "Now that the training dataset has been pruned and transformed into a ML-friendly format, various learning techniques will be employed (and, eventually, the best variant of the best learning type will be identified).\n",
    "\n",
    "To do so, the following steps will be executed:\n",
    "\n",
    "* Determine candidate methods for dimensionality reduction using stochastic gradient descent\n",
    "* Using the candidates, identify the best regression learning algorithm\n",
    "* Using the best regression learner, re-run candidate manifold learners, and select the best dim. reducer\n",
    "* Optimize hyperparameters using grid-based cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Dimensionality Reduction Technique\n",
    "First, candidate methods to reduce the dimensionality of our dataset will be chosen.\n",
    "\n",
    "Stochastic gradient descent will be used as the baseline model type to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality reduction techniques\n",
    "from sklearn.decomposition  import PCA, KernelPCA\n",
    "from sklearn.manifold       import LocallyLinearEmbedding, MDS, SpectralEmbedding, Isomap, TSNE\n",
    "from sklearn.pipeline       import Pipeline\n",
    "\n",
    "# supervised regression techniques\n",
    "from sklearn.linear_model   import SGDRegressor\n",
    "\n",
    "# if valid, number of iterations to train per epoch + tolerance\n",
    "iterVal=1000\n",
    "toleVal=1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle competition uses the root-mean-squared logarithmic error to rank submissions, but this is not defined in SciKit-Learn. We will manually define this loss function, instead (but in its absolute-value form, so that it can be minimized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer  import make_scorer;   import math\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def ARMSL(truth,pred):\n",
    "    SLE=0\n",
    "    eps=1e-16\n",
    "    for i in np.arange(len(pred)):\n",
    "        if pred[i]+1 > eps:\n",
    "            SLE += (np.log(pred[i]+1) - np.log(truth[i]+1))**2\n",
    "        else:\n",
    "            SLE += (np.log(eps) - np.log(truth[i]+1))**2    # prevent log explosion\n",
    "    return abs(np.sqrt( SLE.mean() ))\n",
    "\n",
    "armsle=make_scorer(ARMSL, greater_is_better=False)\n",
    "\n",
    "def checkARMSLE(MLmodel, df, truth, scoreObj, cv, lim):\n",
    "    scoring={'ARMSLE':scoreObj}\n",
    "    scores=cross_validate(MLmodel, df, truth, scoring=scoring, cv=cv)\n",
    "    arm=-scores['test_ARMSLE'].mean() # get average of abs-root-mean-squared-log errors\n",
    "    armFmt=round(arm, 2)  if arm<lim  else 'above limit'\n",
    "    return armFmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a dictionary of dimensionality reduction techniques to iterate through candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducers={'Regular PCA (95th %)':    PCA(random_state=seedNum, n_components=0.95),\n",
    "          'Kernel PCA (p=3/n=20)':   KernelPCA(random_state=seedNum, kernel='poly',   n_components=20, degree=3, n_jobs=-1),\n",
    "          'Kernel PCA (p=3/n=80)':   KernelPCA(random_state=seedNum, kernel='poly',   n_components=80,degree=3, n_jobs=-1),\n",
    "          'Kernel PCA (cos/n=20)':   KernelPCA(random_state=seedNum, kernel='cosine', n_components=20, n_jobs=-1),\n",
    "          'Kernel PCA (cos/n=80)':   KernelPCA(random_state=seedNum, kernel='cosine', n_components=80,n_jobs=-1),\n",
    "          'Loc. Lin. Emb. (n=30)':   LocallyLinearEmbedding(random_state=seedNum, n_components=30, n_jobs=-1),\n",
    "          'Loc. Lin. Emb. (n=60)':   LocallyLinearEmbedding(random_state=seedNum, n_components=60, n_jobs=-1),\n",
    "          'Loc. Lin. Emb. (n=100)':  LocallyLinearEmbedding(random_state=seedNum, n_components=100, n_jobs=-1),\n",
    "          'MDS (n=10)':              MDS(random_state=seedNum, n_components=10, n_jobs=-1),\n",
    "          'Spec. Emb. (n=20)':       SpectralEmbedding(random_state=seedNum, n_components=20, n_jobs=-1),\n",
    "          'Spec. Emb. (n=50)':       SpectralEmbedding(random_state=seedNum, n_components=50, n_jobs=-1),\n",
    "          'Spec. Emb. (n=100)':      SpectralEmbedding(random_state=seedNum, n_components=100, n_jobs=-1),\n",
    "          't-SNE (rand/p=70)':       TSNE(random_state=seedNum, perplexity=70, init='random'),\n",
    "          'Isomap (n=20)':           Isomap(n_components=20, n_jobs=-1),\n",
    "          'Isomap (n=70)':           Isomap(n_components=70, n_jobs=-1)}\n",
    "\n",
    "# define ML regressor for reference\n",
    "MLbase=SGDRegressor(random_state=seedNum, max_iter=iterVal, tol=toleVal)\n",
    "#MLbase=LassoCV(random_state=seedNum, max_iter=iterVal, tol=toleVal)\n",
    "\n",
    "print('Dictionary creation complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each method in the dictionary will be iterated through, and each iteration of stochastic gradient descent will output a mean absolute error for house cost predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numxVal=10 # number of folds for cross-validation\n",
    "\n",
    "import time\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# initialize dataframe to store ML parameters\n",
    "resultLabel=(['Pipeline', 'Abs RMS Log Error', 'Training Time (s)'])\n",
    "results=pd.DataFrame(columns=resultLabel)\n",
    "\n",
    "# suppress warnings about convergence\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "# first, attempt a round of ML training without applying ANY dim. reduction\n",
    "t0=time.time()                            # start measuring time to get thru this epoch\n",
    "MLbase.fit(ds, truth)                     # fit model\n",
    "MLbase.predict(ds)                        # predict\n",
    "tTrain=round(time.time()-t0, 1)           # get time needed to calculate\n",
    "armKa=checkARMSLE(MLbase, ds, truth, armsle, numxVal, 0.3)\n",
    "resultNow=[['No Reduction', armKa, tTrain]]\n",
    "results=results.append(pd.DataFrame(resultNow,columns=resultLabel),ignore_index=True)\n",
    "\n",
    "for redname,rednow in zip(reducers.keys(), reducers.values()):\n",
    "    t0=time.time()                        # start measuring time to get thru this epoch\n",
    "    ds_red=rednow.fit_transform(ds)       # apply current transform technique\n",
    "    MLbase.fit(ds_red, truth)             # fit model\n",
    "    MLbase.predict(ds_red)                # predict\n",
    "    tTrain=round(time.time()-t0, 1)       # get time needed to calculate\n",
    "    armKa=checkARMSLE(MLbase, ds_red, truth, armsle, numxVal, 1)\n",
    "    resultNow=[[redname, armKa, tTrain]]  # convert result into dataframe\n",
    "    results=results.append(pd.DataFrame(resultNow,columns=resultLabel),ignore_index=True)\n",
    "    print('...', end='')                  # just keep loading, just keep loading...\n",
    "\n",
    "print(); print('Training complete! See below for rough performance result:')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results suggest that cosine-kernel PCA is optimal for this application.\n",
    "\n",
    "The above learning methods will be trained and cross-validated; this will allow for easy discrimination of the rough performance of the model in the given environment.\n",
    "\n",
    "### Select Model Type\n",
    "Next, the best model type and corresponding reduction technique will be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import more learning techniques\n",
    "from sklearn.neighbors      import KNeighborsRegressor;      from sklearn.svm  import SVR\n",
    "from sklearn.neural_network import MLPRegressor;             from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model   import ElasticNet,Ridge,Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducer={'Loc. Lin. Emb. (n=20)': LocallyLinearEmbedding(random_state=seedNum, n_components=20, n_jobs=-1),\n",
    "#         'Loc. Lin. Emb. (n=21)': LocallyLinearEmbedding(random_state=seedNum, n_components=21, n_jobs=-1),\n",
    "#         'Loc. Lin. Emb. (n=22)': LocallyLinearEmbedding(random_state=seedNum, n_components=22, n_jobs=-1)}\n",
    "reducer=KernelPCA(random_state=seedNum, kernel='cosine', n_components=80, n_jobs=-1)\n",
    "\n",
    "models={'Ridge Regressor':       Ridge(random_state=seedNum),\n",
    "        'Elastic Net':           ElasticNet(random_state=seedNum),\n",
    "        'Lasso Regressor':       Lasso(random_state=seedNum),\n",
    "        'Decision Tree':         DecisionTreeRegressor(random_state=seedNum),\n",
    "        'Support Vector Machine':SVR(max_iter=iterVal,tol=toleVal,gamma='scale'),\n",
    "        'k-Nearest Neighbors':   KNeighborsRegressor(),\n",
    "        'Fully-Connect. N. Net.':MLPRegressor(random_state=seedNum,solver='lbfgs')}\n",
    "\n",
    "print('Dictionary creation complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numxVal=10 # number of folds for cross-validation\n",
    "\n",
    "resultLabel=(['Reducer','Pipeline','RMS Log Error','Training Time (s)'])\n",
    "results=pd.DataFrame(columns=resultLabel)     # initialize storage dataframe, again\n",
    "\n",
    "for MLname,MLnow in zip(models.keys(),  models.values()):\n",
    "    noRedDone=False                       # reset flag to trigger no-preprocessing analysis\n",
    "        \n",
    "    if noRedDone is False:\n",
    "        noRedDone=True                    # set flag to only skip dim. reduc. once per model type\n",
    "        t0=time.time()                    # start measuring time to get thru all epochs\n",
    "        MLnow.fit(ds_red, truth)\n",
    "        MLnow.predict(ds_red)\n",
    "        tTrain=round(time.time()-t0, 2)   # get time needed to calculate\n",
    "        armKa=checkARMSLE(MLnow, ds_red, truth, armsle, numxVal, 0.3)\n",
    "        resultNow=[['No Reduction', MLname, armKa, tTrain]]  # convert result into dataframe\n",
    "        results=results.append(pd.DataFrame(resultNow,columns=resultLabel),ignore_index=True)\n",
    "\n",
    "    t0=time.time()                        # start measuring time to get thru all epochs\n",
    "    ds_red=reducer.fit_transform(ds)      # apply current transform technique\n",
    "    MLnow.fit(ds_red, truth)              # fit model\n",
    "    MLnow.predict(ds_red)                 # predict\n",
    "    tTrain=round(time.time()-t0, 2)       # get time needed to calculate\n",
    "    armKa=checkARMSLE(MLnow, ds_red, truth, armsle, numxVal, 0.3)\n",
    "\n",
    "    resultNow=[['Cosine kPCA', MLname, armKa, tTrain]]  # convert result into dataframe\n",
    "    results=results.append(pd.DataFrame(resultNow,columns=resultLabel),ignore_index=True)\n",
    "    print('..', end='')                  # just keep loading, just keep loading...\n",
    "        \n",
    "        \n",
    "print(); print('Training complete! See below for rough performance result:')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these results, cosine-kernel PCA combined with algorithms other than linear regressors (i.e. not lasso/ridge/elastic net) appears to give the lowest mean absolute error in housing price estimates.\n",
    "\n",
    "The distribution of inputted features using this manifold transformer is illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "reducer=KernelPCA(random_state=seedNum, kernel='cosine', n_components=80, n_jobs=-1)\n",
    "ds_red=reducer.fit_transform(ds)\n",
    "ds_red=pd.DataFrame(ds_red, index=ds.index) # makee reduced dataset into dataframe\n",
    "\n",
    "#sctr=scatter_matrix(ds_red, figsize=(12,12), range_padding=0.6, diagonal='kde')\n",
    "ax=ds_red.plot.density(figsize=(12,3), alpha=0.4, legend=False, xlim=(-0.2, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the optimal regression algorithm appears to be inconclusive from an accuracy perspective. In the interest of avoiding algorithms that are prone to overfitting, only the support vector machine, k-nearest neighbors, and perceptron approaches will be kept for further analysis.\n",
    "\n",
    "## Optimize training\n",
    "The hyperparameters of the above functions will be tuned via grid-search cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models={'k-Nearest Neighbors': KNeighborsRegressor(),\n",
    "        'Support Vector Mch.': SVR()}\n",
    "\n",
    "params=[{'n_neighbors':       [3, 5, 7, 10, 15, 20],\n",
    "         'algorithm':         ['ball_tree','kd_tree'],\n",
    "         'leaf_size':         [10, 20, 30, 40],\n",
    "         'weights':           ['uniform','distance']},\n",
    "        {'kernel':            ['poly','sigmoid','rbf'],\n",
    "         'gamma':             [0.01,0.05,0.1,0.2,'scale'],\n",
    "         'epsilon':           [0.1, 0.05, 0.01]},]\n",
    "\n",
    "print('Regression optimization setup complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "numxVal=10 # number of folds for cross-validation\n",
    "\n",
    "modelID=0\n",
    "for MLname,MLnow in zip(models.keys(),  models.values()):\n",
    "    paramNow=params[modelID]              # get parameters for current model\n",
    "    \n",
    "    ds_red=reducer.fit_transform(ds)      # reduce dimensions\n",
    "    searchMe=GridSearchCV(MLnow, param_grid=paramNow, cv=numxVal, scoring=armsle, return_train_score=True)\n",
    "    \n",
    "    print('Testing',MLname,'...')\n",
    "    \n",
    "    searchMe.fit(ds_red, truth)           # try current set of hyperparameters\n",
    "    error_avg=abs(searchMe.cv_results_['mean_test_score'].mean())\n",
    "    error_std=np.sqrt(sum(searchMe.cv_results_['std_test_score']**2))\n",
    "    \n",
    "    modelID=modelID+1                     # manually iterate to prepare for next model\n",
    "    \n",
    "    print('Best Parameters for',MLname,':',searchMe.best_params_)\n",
    "    print(\"Abs RMS log error =\", round(error_avg,2), '±', round(error_std,2))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Analysis of perceptrons skipped due to time constraints')\n",
    "#params={'hidden_layer_sizes':[(30,),(50,),(70),(100)],\n",
    "#        'activation':        ['tanh','relu'],\n",
    "#        'solver':            ['lbfgs','sgd','adam'],\n",
    "#        'learning_rate':     ['constant','invscaling']}\n",
    "\n",
    "# test perceptron layer dimensions separately, in thee interest of time\n",
    "#NN=MLPRegressor(random_state=seedNum)\n",
    "\n",
    "#t0=time.time()                        # start measuring time to get thru all epochs\n",
    "\n",
    "#for epoch in range(epochs):\n",
    "#    ds_red=reducer.fit_transform(ds)  # reduce dimensions\n",
    "#    searchMe=GridSearchCV(NN, param_grid=params, cv=numxVal, scoring=armsle, return_train_score=True)\n",
    "#    searchMe.fit(ds_red, truth)       # try current set of hyperparameters\n",
    "\n",
    "#error_avg=abs(searchMe.cv_results_['mean_test_score'].mean())\n",
    "#error_std=np.sqrt(sum(searchMe.cv_results_['std_test_score']**2))\n",
    "\n",
    "#print('Best Parameters for',MLname,':',searchMe.best_params_)\n",
    "#print(\"Abs RMS log error =\", round(error_avg,2), '±', round(error_std,2))\n",
    "#print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply desired model to test dataset\n",
    "Define an ML model and its hyperparameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.manifold  import LocallyLinearEmbedding\n",
    "\n",
    "epochs=20  # number of epochs to train\n",
    "\n",
    "reducer=KernelPCA(random_state=seedNum, kernel='cosine', n_components=80, n_jobs=-1)\n",
    "\n",
    "MLfinal=KNeighborsRegressor(algorithm='ball_tree', leaf_size=10, n_neighbors=5, weights='distance')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Training epoch',epoch+1,'of',epochs)\n",
    "    ds_red=reducer.fit_transform(ds) # reduce dimensions\n",
    "    MLfinal.fit(ds_red,truth)        # fit models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and apply it to the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest=pd.read_csv('../Datasets/housePrices/test.csv')\n",
    "\n",
    "dtest.set_index('Id', inplace=True)              # make entry ID into dataframe index\n",
    "\n",
    "dt=organize(dtest)                               # encode test dataset\n",
    "dt_scaled=scaler.transform(dt.values)            # apply robust scalar (see training section)\n",
    "dt_sca=pd.DataFrame(dt_scaled, index=dt.index, columns=dt.columns)\n",
    "dt_red=reducer.transform(dt_sca)                 # reduce dimensions\n",
    "\n",
    "dtest_out=MLfinal.predict(dt_red)                # apply desired ML model to test dataset\n",
    "\n",
    "dtest_out=np.exp(dtest_out)                      # undo log-normalization of output\n",
    "dtest_out=pd.DataFrame(dtest_out, index=dt.index, columns=['BaseVal']) # re-format as dataframe\n",
    "dtest_adj=org_considerMisc(dtest_out, dtest)     # add miscellaneous features' dollar values\n",
    "dtest_adj.to_csv('test-output.csv', header=True) # save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
